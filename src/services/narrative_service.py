"""Narrative service for parsing and retrieving facility contribution narratives.

This service loads narrative markdown files generated by the insight graph runtime,
parses them into structured data models, and provides query methods for retrieval.
Narratives contain rich contribution analysis including Pareto analysis, driver
rankings, and hierarchical breakdowns.

Usage:
    service = NarrativeService()
    insights = service.get_narrative("AFP658")
    if insights:
        print(f"LOS Index: {insights.metric_value}")
        print(f"Top contributor: {insights.top_drivers.higher_los[0].segment}")

Author: Quality Compass Team
"""

from __future__ import annotations

import logging
import re
from dataclasses import dataclass, field
from datetime import UTC, datetime
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)

# Default paths for narrative files
DEFAULT_RUNS_ROOT = Path("runs")
DEFAULT_INSIGHT_GRAPH_RUN = "inpatient_throughput_v2/20251210170210"


# =============================================================================
# Data Models
# =============================================================================


@dataclass
class ContributorSummary:
    """Single contributor in executive summary.

    Attributes:
        dimension: Dimension type (Payer, Discharge, Service Line, etc.).
        segment: Segment name.
        excess: Contribution excess value (+/-).
        trend: Trend indicator (â†‘, â†“, â†’) with percentage.
        flags: Optional flags (Double, Triple, etc.).
    """

    dimension: str
    segment: str
    excess: float
    trend: str | None = None
    flags: list[str] = field(default_factory=list)


@dataclass
class ExecutiveSummary:
    """Executive summary extracted from narrative.

    Attributes:
        metric_value: Primary metric value.
        total_segments: Total segments analyzed.
        facility_segments: Facility-level segments count.
        pareto_insight: Top N segments account for X% insight.
        top_contributors_higher: Top 3 contributors to higher LOS.
        top_contributors_lower: Top 3 contributors to lower LOS.
    """

    metric_value: float
    total_segments: int
    facility_segments: int
    pareto_insight: str
    top_contributors_higher: list[ContributorSummary]
    top_contributors_lower: list[ContributorSummary]


@dataclass
class MetricComparison:
    """Single metric in cross-metric comparison.

    Attributes:
        metric_name: Display name of the metric.
        value: Metric value.
        z_score: Z-score vs peers.
        peer_status: Peer status label (e.g., "moderately high").
    """

    metric_name: str
    value: float
    z_score: float
    peer_status: str


@dataclass
class ParetoSegment:
    """Single segment in Pareto analysis.

    Attributes:
        segment_name: Name of the segment.
        excess: Contribution excess value.
        cumulative_pct: Cumulative percentage of total.
    """

    segment_name: str
    excess: float
    cumulative_pct: float


@dataclass
class ParetoAnalysis:
    """Pareto analysis data.

    Attributes:
        positive_excess: Segments adding to metric (worse).
        negative_excess: Segments reducing metric (better).
        top_n_positive_pct: Top N positive account for X%.
        top_n_negative_pct: Top N negative account for X%.
    """

    positive_excess: list[ParetoSegment]
    negative_excess: list[ParetoSegment]
    top_n_positive_pct: float
    top_n_negative_pct: float


@dataclass
class Driver:
    """Single driver in top drivers table.

    Attributes:
        rank: Ranking position.
        dimension: Dimension type.
        segment: Segment name.
        value: Metric value for segment.
        weight: Segment weight percentage.
        excess: Contribution excess.
        trend: Trend indicator with percentage.
        slope_percentile: Slope percentile (0-100).
        mean_z: Mean z-score over period.
        z_score: Current z-score.
        peer_status: Peer status label.
        multi_kpi: Multi-KPI flag (Double/Triple/-).
        interpretation: Human-readable interpretation.
        cumulative_pct: Cumulative contribution percentage.
    """

    rank: int
    dimension: str
    segment: str
    value: float
    weight: float
    excess: float
    trend: str | None
    slope_percentile: int | None
    mean_z: float | None
    z_score: float
    peer_status: str
    multi_kpi: str | None
    interpretation: str
    cumulative_pct: float


@dataclass
class TopDrivers:
    """Top drivers data container.

    Attributes:
        higher_los: Drivers contributing to higher LOS.
        lower_los: Drivers contributing to lower LOS.
    """

    higher_los: list[Driver]
    lower_los: list[Driver]


@dataclass
class InsightItem:
    """Single insight item.

    Attributes:
        segment: Segment name.
        excess: Contribution excess.
        z_score: Z-score vs peers.
        peer_status: Peer status label.
    """

    segment: str
    excess: float
    z_score: float
    peer_status: str


@dataclass
class InsightCategories:
    """Categorized insights.

    Attributes:
        double_trouble: High excess AND unusual vs peers.
        internal_issue: High excess but normal vs peers.
    """

    double_trouble: list[InsightItem]
    internal_issue: list[InsightItem]


@dataclass
class HierarchyNode:
    """Node in hierarchical breakdown.

    Attributes:
        level: Level type (SL=Service Line, SSL=Sub-Service Line).
        segment: Segment name.
        value: Metric value.
        weight: Segment weight percentage.
        excess: Contribution excess.
        z_score: Z-score vs peers.
        peer_status: Peer status label.
        interpretation: Human-readable interpretation.
        children: Child nodes (for SL level).
    """

    level: str  # "SL" or "SSL"
    segment: str
    value: float
    weight: float
    excess: float
    z_score: float
    peer_status: str
    interpretation: str
    children: list[HierarchyNode] = field(default_factory=list)


@dataclass
class NarrativeInsights:
    """Container for all extracted narrative insights.

    Attributes:
        facility_id: Medicare facility identifier.
        metric_value: Primary metric value (e.g., LOS Index).
        generated_at: Timestamp when narrative was generated.
        executive_summary: Extracted executive summary.
        cross_metric_comparison: Peer comparison data.
        pareto_analysis: Pareto cumulative impact data.
        top_drivers: Ranked driver tables (higher and lower).
        insights: Categorized insight lists.
        hierarchical_breakdown: Service line hierarchy.
        source_file: Path to source markdown file.
    """

    facility_id: str
    metric_value: float
    generated_at: datetime
    executive_summary: ExecutiveSummary
    cross_metric_comparison: list[MetricComparison]
    pareto_analysis: ParetoAnalysis
    top_drivers: TopDrivers
    insights: InsightCategories
    hierarchical_breakdown: list[HierarchyNode]
    source_file: Path | None = None


# =============================================================================
# Exceptions
# =============================================================================


class NarrativeServiceError(Exception):
    """Exception raised when narrative service operations fail.

    Attributes:
        message: Error description.
        facility_id: Facility that was being processed.
    """

    def __init__(
        self,
        message: str,
        facility_id: str | None = None,
    ) -> None:
        """Initialize the error.

        Args:
            message: Error description.
            facility_id: Facility that was being processed.
        """
        self.message = message
        self.facility_id = facility_id
        super().__init__(f"{message} (facility={facility_id})")


# =============================================================================
# Service Implementation
# =============================================================================


class NarrativeService:
    """Service for parsing and retrieving narrative insights.

    This service loads narrative markdown files, parses them into
    structured data models, and provides query methods for retrieval.

    Example:
        >>> service = NarrativeService()
        >>> insights = service.get_narrative("AFP658")
        >>> if insights:
        ...     print(f"Metric: {insights.metric_value}")
        ...     for d in insights.top_drivers.higher_los[:3]:
        ...         print(f"  {d.segment}: {d.excess:+.4f}")
    """

    def __init__(
        self,
        runs_root: Path | None = None,
        insight_graph_run: str | None = None,
        project_root: Path | None = None,
    ) -> None:
        """Initialize the narrative service.

        Args:
            runs_root: Root directory for run outputs.
            insight_graph_run: Relative path to insight graph run.
            project_root: Project root directory for resolving relative paths.

        Raises:
            None: Initialization does not raise.
        """
        self._project_root = project_root or Path.cwd()
        self._runs_root = runs_root or (self._project_root / DEFAULT_RUNS_ROOT)
        self._insight_graph_run = insight_graph_run or DEFAULT_INSIGHT_GRAPH_RUN
        self._narrative_dir = self._runs_root / self._insight_graph_run / "analysis" / "narrative"

    def get_narrative(
        self,
        facility_id: str,
    ) -> NarrativeInsights | None:
        """Get narrative insights for a facility.

        Args:
            facility_id: Medicare facility identifier.

        Returns:
            NarrativeInsights if found, None otherwise.

        Raises:
            NarrativeServiceError: If parsing fails.
        """
        file_path = self._narrative_dir / f"contribution_{facility_id}.md"

        if not file_path.exists():
            logger.warning("Narrative file not found: %s", file_path)
            return None

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()
            return self.parse_markdown(content, file_path)
        except OSError as e:
            raise NarrativeServiceError(
                f"Failed to read narrative file: {e}",
                facility_id=facility_id,
            ) from e

    def get_executive_summary(
        self,
        facility_id: str,
    ) -> ExecutiveSummary | None:
        """Get executive summary only (lightweight).

        Args:
            facility_id: Medicare facility identifier.

        Returns:
            ExecutiveSummary if found, None otherwise.

        Raises:
            NarrativeServiceError: If parsing fails.
        """
        insights = self.get_narrative(facility_id)
        if insights:
            return insights.executive_summary
        return None

    def list_available_facilities(self) -> list[str]:
        """List facilities with narrative data.

        Returns:
            List of facility IDs with available narratives.

        Raises:
            None: Returns empty list if directory doesn't exist.
        """
        if not self._narrative_dir.exists():
            logger.warning("Narrative directory not found: %s", self._narrative_dir)
            return []

        facilities = []
        for file_path in self._narrative_dir.glob("contribution_*.md"):
            # Extract facility ID from filename: contribution_AFP658.md -> AFP658
            match = re.match(r"contribution_(.+)\.md", file_path.name)
            if match:
                facilities.append(match.group(1))

        return sorted(facilities)

    def parse_markdown(
        self,
        markdown_content: str,
        source_file: Path | None = None,
    ) -> NarrativeInsights:
        """Parse markdown content into structured insights.

        Args:
            markdown_content: Raw markdown string.
            source_file: Optional source file path for traceability.

        Returns:
            Parsed NarrativeInsights object.

        Raises:
            NarrativeServiceError: If parsing fails critically.
        """
        try:
            # Parse header
            facility_id, metric_value, generated_at = _parse_header(markdown_content)

            # Split into sections
            sections = _split_sections(markdown_content)

            # Parse each section
            executive_summary = _parse_executive_summary(
                sections.get("Executive Summary", ""),
                metric_value,
            )
            cross_metric = _parse_cross_metric_table(sections.get("Cross-Metric Peer Comparison", ""))
            pareto = _parse_pareto_analysis(sections.get("Pareto Analysis: Cumulative Impact", ""))
            higher_drivers = _parse_drivers_table(sections.get("Top Drivers of Higher LOS (Positive Excess)", ""))
            lower_drivers = _parse_drivers_table(sections.get("Top Drivers of Lower LOS (Negative Excess)", ""))
            insights = _parse_insights_section(sections.get("Insights: Internal vs External Comparison", ""))
            hierarchy = _parse_hierarchy_table(sections.get("Hierarchical Contribution Breakdown", ""))

            return NarrativeInsights(
                facility_id=facility_id,
                metric_value=metric_value,
                generated_at=generated_at,
                executive_summary=executive_summary,
                cross_metric_comparison=cross_metric,
                pareto_analysis=pareto,
                top_drivers=TopDrivers(
                    higher_los=higher_drivers,
                    lower_los=lower_drivers,
                ),
                insights=insights,
                hierarchical_breakdown=hierarchy,
                source_file=source_file,
            )

        except Exception as e:
            facility_id_from_file = None
            if source_file:
                match = re.search(r"contribution_(.+)\.md", source_file.name)
                if match:
                    facility_id_from_file = match.group(1)

            raise NarrativeServiceError(
                f"Failed to parse narrative: {e}",
                facility_id=facility_id_from_file,
            ) from e


# =============================================================================
# Internal Parsing Functions
# =============================================================================


def _parse_header(content: str) -> tuple[str, float, datetime]:
    """Extract facility_id, metric_value, generated_at from header.

    Args:
        content: Full markdown content.

    Returns:
        Tuple of (facility_id, metric_value, generated_at).

    Raises:
        ValueError: If header parsing fails.
    """
    # Extract facility ID from title: "# Contribution Analysis: Medicare ID AFP658"
    facility_match = re.search(r"# Contribution Analysis: Medicare ID (\w+)", content)
    if not facility_match:
        raise ValueError("Could not parse facility ID from header")
    facility_id = facility_match.group(1)

    # Extract metric value: "**Facility LOS Index**: 1.1668"
    metric_match = re.search(r"\*\*Facility LOS Index\*\*:\s*([\d.]+)", content)
    metric_value = float(metric_match.group(1)) if metric_match else 0.0

    # Extract generated timestamp: "**Generated**: 2025-12-13 16:46:14 UTC"
    gen_match = re.search(r"\*\*Generated\*\*:\s*(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) UTC", content)
    generated_at = datetime.strptime(gen_match.group(1), "%Y-%m-%d %H:%M:%S").replace(tzinfo=UTC) if gen_match else datetime.now(tz=UTC)

    return facility_id, metric_value, generated_at


def _split_sections(content: str) -> dict[str, str]:
    """Split markdown into sections by ## headers.

    Args:
        content: Full markdown content.

    Returns:
        Dict mapping section title to section content.
    """
    sections: dict[str, str] = {}
    # Match ## headers but not ### or deeper
    pattern = r"^## (.+?)$"
    matches = list(re.finditer(pattern, content, re.MULTILINE))

    for i, match in enumerate(matches):
        title = match.group(1).strip()
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        sections[title] = content[start:end].strip()

    return sections


def _parse_executive_summary(section: str, metric_value: float) -> ExecutiveSummary:
    """Parse executive summary section.

    Args:
        section: Executive summary section content.
        metric_value: Metric value from header.

    Returns:
        Parsed ExecutiveSummary object.
    """
    # Extract total segments: "**Total segments analyzed**: 213 (69 facility-level)"
    total_match = re.search(r"\*\*Total segments analyzed\*\*:\s*(\d+)\s*\((\d+)", section)
    total_segments = int(total_match.group(1)) if total_match else 0
    facility_segments = int(total_match.group(2)) if total_match else 0

    # Extract pareto insight
    pareto_match = re.search(r"\*\*Pareto Insight\*\*:\s*(.+)", section)
    pareto_insight = pareto_match.group(1).strip() if pareto_match else ""

    # Parse contributors
    higher_contributors = _parse_contributors(section, "HIGHER LOS")
    lower_contributors = _parse_contributors(section, "LOWER LOS")

    return ExecutiveSummary(
        metric_value=metric_value,
        total_segments=total_segments,
        facility_segments=facility_segments,
        pareto_insight=pareto_insight,
        top_contributors_higher=higher_contributors,
        top_contributors_lower=lower_contributors,
    )


def _parse_contributors(section: str, direction: str) -> list[ContributorSummary]:
    """Parse contributor list from executive summary.

    Args:
        section: Executive summary section content.
        direction: Either "HIGHER LOS" or "LOWER LOS".

    Returns:
        List of ContributorSummary objects.
    """
    contributors: list[ContributorSummary] = []

    # Find the relevant subsection
    pattern = rf"\*\*Top contributors to {direction}\*\*.*?(?=\*\*Top contributors|\n---|\Z)"
    match = re.search(pattern, section, re.DOTALL | re.IGNORECASE)
    if not match:
        return contributors

    subsection = match.group(0)

    # Parse each contributor line: "  - [Discharge] Segment name: +0.3117, - -79%"
    line_pattern = r"^\s*-\s*\[(\w+)\]\s*(.+?):\s*([+-]?[\d.]+)(?:,\s*(.+?))?$"
    for line_match in re.finditer(line_pattern, subsection, re.MULTILINE):
        dimension = line_match.group(1)
        segment = line_match.group(2).strip()
        excess = float(line_match.group(3))
        trend = line_match.group(4).strip() if line_match.group(4) else None

        contributors.append(
            ContributorSummary(
                dimension=dimension,
                segment=segment,
                excess=excess,
                trend=trend,
                flags=[],
            )
        )

    return contributors


def _parse_cross_metric_table(section: str) -> list[MetricComparison]:
    """Parse cross-metric peer comparison table.

    Args:
        section: Cross-metric section content.

    Returns:
        List of MetricComparison objects.
    """
    comparisons: list[MetricComparison] = []

    # Parse markdown table rows
    # | Mean ICU Days | 6.640 | +1.38 | moderately high |
    pattern = r"\|\s*([^|]+)\s*\|\s*([\d.]+)\s*\|\s*([+-]?[\d.]+)\s*\|\s*([^|]+)\s*\|"
    for match in re.finditer(pattern, section):
        metric_name = match.group(1).strip()
        # Skip header row
        if metric_name.lower() in ("metric", "-----", ""):
            continue

        try:
            comparisons.append(
                MetricComparison(
                    metric_name=metric_name,
                    value=float(match.group(2)),
                    z_score=float(match.group(3)),
                    peer_status=match.group(4).strip(),
                )
            )
        except ValueError:
            continue  # Skip malformed rows

    return comparisons


def _parse_pareto_analysis(section: str) -> ParetoAnalysis:
    """Parse Pareto analysis section with ASCII charts.

    Args:
        section: Pareto analysis section content.

    Returns:
        Parsed ParetoAnalysis object.
    """
    positive_segments: list[ParetoSegment] = []
    negative_segments: list[ParetoSegment] = []

    # Split into positive and negative sections
    positive_section = ""
    negative_section = ""

    if "### Segments Adding" in section:
        pos_match = re.search(
            r"### Segments Adding.*?```(.+?)```",
            section,
            re.DOTALL,
        )
        if pos_match:
            positive_section = pos_match.group(1)

    if "### Segments Reducing" in section:
        neg_match = re.search(
            r"### Segments Reducing.*?```(.+?)```",
            section,
            re.DOTALL,
        )
        if neg_match:
            negative_section = neg_match.group(1)

    # Parse ASCII chart lines
    # â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   14.9% | Segment Name (+0.3117)
    chart_pattern = r"[â–“â–‘]+\s+([\d.]+)%\s*\|\s*(.+?)\s+\(([+-]?[\d.]+)\)"

    for match in re.finditer(chart_pattern, positive_section):
        positive_segments.append(
            ParetoSegment(
                segment_name=match.group(2).strip(),
                excess=float(match.group(3)),
                cumulative_pct=float(match.group(1)),
            )
        )

    for match in re.finditer(chart_pattern, negative_section):
        negative_segments.append(
            ParetoSegment(
                segment_name=match.group(2).strip(),
                excess=float(match.group(3)),
                cumulative_pct=float(match.group(1)),
            )
        )

    # Extract top N percentages
    top_pos_match = re.search(r"\[Top \d+ = (\d+)% of total positive", section)
    top_neg_match = re.search(r"\[Top \d+ = (\d+)% of total negative", section)

    return ParetoAnalysis(
        positive_excess=positive_segments,
        negative_excess=negative_segments,
        top_n_positive_pct=float(top_pos_match.group(1)) if top_pos_match else 0.0,
        top_n_negative_pct=float(top_neg_match.group(1)) if top_neg_match else 0.0,
    )


def _parse_drivers_table(section: str) -> list[Driver]:
    """Parse top drivers markdown table.

    Args:
        section: Drivers table section content.

    Returns:
        List of Driver objects.
    """
    drivers: list[Driver] = []

    # Find table rows (skip header and separator)
    # | Rank | Dimension | Segment | LOS Index (Agg) | Weight | Excess (Agg) | Trend (12mo) | Slope %ile | Mean Z (12mo) | Z (Agg) | Peer Status | Multi-KPI | Interpretation |
    lines = section.split("\n")
    in_table = False

    for line in lines:
        if not line.strip().startswith("|"):
            continue

        # Skip header and separator rows
        if "Rank" in line or "---" in line:
            in_table = True
            continue

        if not in_table:
            continue

        # Parse table row
        cells = [c.strip() for c in line.split("|")[1:-1]]  # Remove empty first/last
        if len(cells) < 13:
            continue

        try:
            # Parse trend - can be "â†‘ +68%", "â†“ -75%", "â†’ +22%", "- -79%"
            trend_raw = cells[6].strip() if cells[6].strip() != "-" else None

            # Parse slope percentile - can be integer or "-"
            slope_str = cells[7].strip()
            slope_pct = int(slope_str) if slope_str != "-" else None

            # Parse mean z - can be float or "-"
            mean_z_str = cells[8].strip()
            mean_z = float(mean_z_str) if mean_z_str != "-" else None

            # Parse cumulative percentage from interpretation [Cum:XX%]
            cum_match = re.search(r"\[Cum:(\d+)%\]", cells[12])
            cum_pct = float(cum_match.group(1)) if cum_match else 0.0

            drivers.append(
                Driver(
                    rank=int(cells[0]),
                    dimension=cells[1],
                    segment=cells[2],
                    value=float(cells[3]),
                    weight=float(cells[4].rstrip("%")),
                    excess=float(cells[5]),
                    trend=trend_raw,
                    slope_percentile=slope_pct,
                    mean_z=mean_z,
                    z_score=float(cells[9]),
                    peer_status=cells[10],
                    multi_kpi=cells[11] if cells[11] != "-" else None,
                    interpretation=cells[12],
                    cumulative_pct=cum_pct,
                )
            )
        except (ValueError, IndexError) as e:
            logger.warning("Failed to parse driver row: %s - %s", line, e)
            continue

    return drivers


def _parse_insights_section(section: str) -> InsightCategories:
    """Parse insights categorization section.

    Args:
        section: Insights section content.

    Returns:
        Parsed InsightCategories object.
    """
    double_trouble: list[InsightItem] = []
    internal_issue: list[InsightItem] = []

    # Split by subsection headers
    # ### âš ï¸ Double Trouble (High Excess + Unusual vs Peers)
    # ### ðŸ“Š Internal Issue (High Excess, Normal vs Peers)

    double_match = re.search(
        r"###.*Double Trouble.*?\n(.*?)(?=###|\Z)",
        section,
        re.DOTALL,
    )
    internal_match = re.search(
        r"###.*Internal Issue.*?\n(.*?)(?=###|\Z)",
        section,
        re.DOTALL,
    )

    # Parse insight items
    # - **Segment Name**: excess +0.3117, z=+1.80 (moderately high)
    # OR: - Segment Name: excess +0.1634, z=+0.71
    item_pattern = r"^\s*-\s*\*?\*?([^*:]+?)\*?\*?:\s*excess\s*([+-]?[\d.]+),\s*z=([+-]?[\d.]+)(?:\s*\(([^)]+)\))?"

    if double_match:
        for match in re.finditer(item_pattern, double_match.group(1), re.MULTILINE):
            double_trouble.append(
                InsightItem(
                    segment=match.group(1).strip(),
                    excess=float(match.group(2)),
                    z_score=float(match.group(3)),
                    peer_status=match.group(4).strip() if match.group(4) else "",
                )
            )

    if internal_match:
        for match in re.finditer(item_pattern, internal_match.group(1), re.MULTILINE):
            internal_issue.append(
                InsightItem(
                    segment=match.group(1).strip(),
                    excess=float(match.group(2)),
                    z_score=float(match.group(3)),
                    peer_status=match.group(4).strip() if match.group(4) else "",
                )
            )

    return InsightCategories(
        double_trouble=double_trouble,
        internal_issue=internal_issue,
    )


def _parse_hierarchy_table(section: str) -> list[HierarchyNode]:
    """Parse hierarchical breakdown table.

    Args:
        section: Hierarchy section content.

    Returns:
        List of top-level HierarchyNode objects with children.
    """
    nodes: list[HierarchyNode] = []
    current_parent: HierarchyNode | None = None

    # Parse table rows
    # | Level | Segment | Value | Weight | Excess | Z-Score | Peer Status | Interpretation |
    # | **SL** | **Neurosurgery** | **3.636** | **5.8%** | **+0.1055** | **+1.57** | **moderately high** | **212% longer...** |
    # | â””â”€ SSL | Neurosurgery - Open Procedure | 12.495 | 74.5% | +2.0413 | +1.29 | moderately high | 244% longer... |

    lines = section.split("\n")
    in_table = False

    for line in lines:
        if not line.strip().startswith("|"):
            continue

        # Skip header and separator
        if "Level" in line or "---" in line:
            in_table = True
            continue

        if not in_table:
            continue

        cells = [c.strip() for c in line.split("|")[1:-1]]
        if len(cells) < 8:
            continue

        try:
            # Determine level from first cell
            level_cell = cells[0].replace("*", "").strip()
            is_ssl = "â””â”€" in level_cell or "SSL" in level_cell

            # Clean values (remove ** bold markers)
            def clean(s: str) -> str:
                return s.replace("*", "").strip()

            segment = clean(cells[1])
            value = float(clean(cells[2]))
            weight = float(clean(cells[3]).rstrip("%"))
            excess = float(clean(cells[4]))
            z_score = float(clean(cells[5]))
            peer_status = clean(cells[6])
            interpretation = clean(cells[7])

            node = HierarchyNode(
                level="SSL" if is_ssl else "SL",
                segment=segment,
                value=value,
                weight=weight,
                excess=excess,
                z_score=z_score,
                peer_status=peer_status,
                interpretation=interpretation,
                children=[],
            )

            if is_ssl and current_parent:
                # Add as child of current parent
                current_parent.children.append(node)
            else:
                # New service line (top level)
                nodes.append(node)
                current_parent = node

        except (ValueError, IndexError) as e:
            logger.warning("Failed to parse hierarchy row: %s - %s", line, e)
            continue

    return nodes
